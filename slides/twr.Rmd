---
title: "Automated Scoring of Constructed-Response Science Items: Prospects and Obstacle"
author: "Liu, O. L., et al. (2014)"
date: "February 24 2022"

output:
  xaringan::moon_reader:
    css: xaringan-themer.css 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(xaringanthemer)
# extra_css <- list(
#   ".red"   = list(color = "red"),
#   ".small" = list("font-size" = "70%"),
#   ".big" = list("font-size" = "120%"),
#   ".full-width" = list(
#     display = "flex",
#     width   = "100%",
#     flex    = "1 1 auto"
#   )
# )

style_mono_light(
  base_color = "#126c78",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```



# Introduction

### Content Based Scoring


-  Content-based scoring refers to the type of scoring that evaluates the content of the responses, as opposed to the writing quality (e.g., essay).


### Concept Based Scoring
- content-based scoring that do not involve the scoring of individual concepts. 

--
- **Analytical Scoring!**

---

# Why Automate Scoring of Constructed Response?

- Reduce the number of human scorers.

- Save on costs for training human scorers.

- Reduce the interval between test administration and score assignment.

- Improve the scoring consistency .

---
# Reproducibility considerations

### Reproducibililty of demographic estimates should be of primary concern


- Used in public and health policy (e.g. things like mortality rates, but also population counts)


- Used in other fields of study (control for age, race/ethnicity)


#### Attitude is often 'taken as given' even though that isn't the case

--

- We built this city (field) on theoretical and empirical approximations

- Reproducibility is important to understand propagation of error and how demographic regularities may change over time

---


# Method

#### Item Selection

- Four constructed response science items

  - **1** Stand alone  
  (Photosynthesis)
  
  - **3** Follow-up to preceding multiple choice item   
  (Thermodynamics and Global Climate Change)

--

#### Holistic Scoring
1. Off task

2. No Link

3. Partial Link

4. Full Link

5. Complex Link

---
# Method

#### Analytical Scoring

- Six concept (**C1 - C6**)

--

##### Scoring Rule

**4** points C1 and (C2 or C3 or C4)  

**3** points {C1 and (C2 or C3 or C4)} and C5  

**3** points (C1 or C2 or C3)  

**2** points (C1 or C2 or C3) and C5  

**2** points C4  

**1** point C4 and C5  

**1** point C5 or C6 or none 

---

# Method

#### Human Raters  
$75\%$ of available responses were scored by humans

  - Postdoctoral Researchers

  - Research Scientist

  - Advance Graduate Students

---

# Method

#### Automated Scoring Tool (*c-Rater*)

.center[Evaluation Criteria ]

| Quadratic Kappa | Correlation | Degradation | Standardized Mean Difference |
| :---        |    :----   |          :--- | :---|
|   Quadratic-weighted kappa coefficient indicates the proportion of agreement among multiple raters|Evaluate the consistency between human and machine scores|Examines the difference between human/machine score agreement and human/human score agreement|Mean difference between human and machine scores divided by the pooled standard deviations|
| Poor (≤.00), slight (.00–.20), fair (.21–.40), moderate (.41–.60), good (.61–.80), and very good (.81–1). **Landis and Koch (1977)**   |  None (0–.09), small (.10–.30), moderate (.31–.50), and large (.51–1.00). **Cohen (1968)**         |  should not be greater than .10 in either kappa or Pearson correlation. **(Williamson et al., 2012)**   |   The standardized mean score difference should not be greater than .15 **(Williamson et al., 2012)**|


---

# Results

# Thank You

.center[![](https://media.giphy.com/media/QyJ0We4GHpjBa7PvKL/giphy.gif)]
